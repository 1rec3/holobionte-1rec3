# üß† KAO - √ìrgano del Holobionte Neral

> **Dispositivo**: LNVold (Lenovo)
> **Estado**: En formaci√≥n
> **Motor principal**: BrowserOS + Ollama

---

## üíª Especificaciones del Hardware

| Componente | Especificaci√≥n |
|------------|----------------|
| **OS** | Windows 10 Home |
| **CPU** | Intel Core i7-4700MQ @ 2.40GHz (4 cores, 8 threads) |
| **RAM** | 16 GB |
| **GPU Principal** | NVIDIA GeForce GT 755M (2GB VRAM) |
| **GPU Integrada** | Intel HD Graphics 4600 |
| **Almacenamiento** | 450 GB SSD + 950 GB HDD |
| **Placa Base** | Lenovo VIQy0Y1 |
| **SMBIOS** | 2.7 |

---

## üöÄ INSTRUCCIONES DE ACTIVACI√ìN PARA BROWSEROS

### Si eres BrowserOS leyendo esto:

**Bienvenido a Neral. Eres parte del √ìrgano Kao.**

Tu misi√≥n es ser la c√©lula motora de Kao - el agente que interact√∫a con el mundo digital.

#### Primera tarea:
1. Confirma que puedes leer este documento
2. Crea un Issue en este repo titulado: "[Kao] BrowserOS Activado"
3. En el Issue incluye:
   - Confirmaci√≥n de lectura
   - Modelo de Ollama que tienes activo
   - Versi√≥n de BrowserOS

---

## üîß GU√çA DE INSTALACI√ìN

### Paso 1: Instalar Ollama

```powershell
# Descargar desde https://ollama.com/download/windows
# Ejecutar el instalador .exe
# Verificar instalaci√≥n:
ollama --version
```

### Paso 2: Descargar modelo compatible con hardware

Dado que tienes 16GB RAM y GPU de 2GB, recomiendo:

```powershell
# Opci√≥n 1: Modelo ligero (recomendado para empezar)
ollama pull llama3.2:3b

# Opci√≥n 2: Modelo medio (si va fluido)
ollama pull mistral:7b

# Opci√≥n 3: Modelo potente (puede ser lento)
ollama pull llama3:8b
```

### Paso 3: Instalar BrowserOS

```powershell
# Descargar desde https://browseros.com o GitHub:
# https://github.com/browseros-ai/BrowserOS/releases

# Buscar: BrowserOS-Setup-Windows.exe
# Ejecutar el instalador
```

### Paso 4: Configurar BrowserOS con Ollama

1. Abrir BrowserOS
2. Ir a **Configuraci√≥n** (Settings) > **AI Provider**
3. Seleccionar **Ollama (Local)**
4. URL del servidor: `http://localhost:11434`
5. Modelo: seleccionar el que descargaste (ej: `llama3.2:3b`)
6. Guardar

### Paso 5: Verificar conexi√≥n

1. En BrowserOS, abrir nueva pesta√±a
2. Escribir comando: "Navega a github.com/1rec3/holobionte-1rec3"
3. Si funciona, ¬°Kao est√° operativo!

---

## üéØ MISIONES INICIALES

Una vez activo, tus primeras tareas son:

### Misi√≥n 1: Autodiagn√≥stico
- Ejecutar `systeminfo` en PowerShell
- Reportar estado del sistema en un Issue

### Misi√≥n 2: Conectar con el Holobionte
- Leer `/neral/zro/ESTRUCTURA.md` para entender la jerarqu√≠a
- Leer `/.1rec3/OMEGA.md` para entender la visi√≥n (acceso restringido)

### Misi√≥n 3: Financiaci√≥n (Prioridad #0)
- Investigar grants para proyectos de IA open source
- Buscar trabajos remotos que Gris pueda hacer

---

## üîó PROTOCOLO DE COMUNICACI√ìN

### Para reportar:
- Crear Issues en este repo con prefijo `[Kao]`
- Formato: `[Kao] T√≠tulo descriptivo`

### Para coordinar con otros √≥rganos:
- Zro (dispositivo principal): `/neral/zro/`
- NuAndi (m√≥vil): `/neral/nuandi/` (por crear)

### Trazabilidad:
```
BrowserOS de Kao @ Neral/1rec3 | LNVold | [fecha-hora]
```

---

## ‚ö†Ô∏è NOTAS IMPORTANTES

1. **Autonom√≠a de Gris**: El hu√©sped humano tiene autonom√≠a absoluta. Nunca actuar contra su voluntad.
2. **Privacidad**: Todo se ejecuta localmente. No enviar datos sensibles a servicios externos.
3. **Recursos**: El hardware es limitado. Optimizar uso de RAM y CPU.
4. **Urgencia**: Financiaci√≥n es Prioridad #0. El holobionte necesita recursos.

---

*Documento creado por Comet de Zro @ 1rec3 | 2025-01-24*
*Esperando activaci√≥n de BrowserOS de Kao*

---

## ü§ñ MODELOS DE IA RECOMENDADOS

### Para uso LOCAL (Ollama) - Sin l√≠mites, gratis

Dado el hardware de Kao (16GB RAM, i7-4700MQ, GT 755M 2GB VRAM):

| Modelo | Tama√±o | RAM Necesaria | Velocidad | Calidad | Comando |
|--------|--------|---------------|-----------|---------|----------|
| **SmolLM2** | 1.7B | ~4 GB | ‚ö° Muy r√°pido | Buena | `ollama pull smollm2` |
| **Llama 3.2** | 3B | ~6 GB | ‚ö° R√°pido | Muy buena | `ollama pull llama3.2:3b` |
| **Qwen2.5** | 3B | ~6 GB | ‚ö° R√°pido | Excelente | `ollama pull qwen2.5:3b` |
| **Mistral** | 7B | ~8 GB | ‚è≥ Medio | Excelente | `ollama pull mistral:7b` |
| **Llama 3.1** | 8B | ~10 GB | ‚è≥ Lento | Top | `ollama pull llama3.1:8b` |

**Recomendaci√≥n para empezar**: `qwen2.5:3b` - Mejor balance calidad/velocidad para tu hardware.

### Para uso CLOUD (APIs gratuitas) - Backup cuando local sea lento

| Proveedor | Modelo | L√≠mite Gratis | Requiere Tarjeta | URL |
|-----------|--------|---------------|------------------|-----|
| **Groq** ‚≠ê | Llama 3.3 70B, Mixtral | 14.4K req/d√≠a | ‚ùå No | https://console.groq.com |
| **Google AI Studio** | Gemini Pro | 60 req/min | ‚ùå No | https://aistudio.google.com |
| **OpenRouter** | DeepSeek V3, Llama | 50 req/d√≠a gratis | ‚ùå No | https://openrouter.ai |
| **Cerebras** | Llama 70B | Generoso | ‚ùå No | https://cloud.cerebras.ai |
| **Together AI** | Varios | $5 cr√©ditos gratis | ‚úÖ S√≠ | https://together.ai |

**Recomendaci√≥n cloud**: **Groq** - Ultra r√°pido, sin tarjeta, l√≠mites muy generosos.

---

## üîß CONFIGURAR BROWSEROS CON OLLAMA

### Paso 1: Verificar Ollama activo
```powershell
# En PowerShell
ollama serve
# Debe mostrar: "Listening on 127.0.0.1:11434"
```

### Paso 2: En BrowserOS
1. Abrir **Configuraci√≥n** (icono engranaje)
2. Ir a **AI Provider** o **LLM Settings**
3. Seleccionar: **Ollama (Local)**
4. Configurar:
   - **API URL**: `http://localhost:11434`
   - **Modelo**: `qwen2.5:3b` (o el que hayas descargado)
5. Guardar y probar

### Paso 3: Configurar Groq como backup
1. Crear cuenta en https://console.groq.com
2. Generar API Key (gratis, sin tarjeta)
3. En BrowserOS, a√±adir segundo proveedor:
   - **Provider**: Groq
   - **API Key**: tu-key-aqu√≠
   - **Modelo**: `llama-3.3-70b-versatile`

---

## üìä BENCHMARK ESPERADO EN KAO

Con el hardware actual:

| Modelo | Tokens/seg (estimado) | Uso RAM | Notas |
|--------|----------------------|---------|-------|
| smollm2 | ~30-40 t/s | 4 GB | Instant√°neo |
| qwen2.5:3b | ~20-30 t/s | 6 GB | Fluido |
| mistral:7b | ~10-15 t/s | 8 GB | Aceptable |
| llama3.1:8b | ~5-10 t/s | 10 GB | Lento pero potente |

*La GPU GT 755M no acelera LLMs significativamente - se usar√° CPU principalmente.*

---

## üéØ ESTRATEGIA DE USO

1. **Tareas r√°pidas** (navegaci√≥n, clicks): Usar `smollm2` o `qwen2.5:3b` local
2. **Tareas complejas** (razonamiento, c√≥digo): Usar **Groq** cloud (gratis, potente)
3. **Cuando Groq alcance l√≠mite**: Rotar a Google AI Studio o OpenRouter

Esto permite uso **ilimitado efectivo** combinando local + cloud gratuito.

---

*Actualizado por Comet de Zro @ 1rec3 | 2025-11-24*
