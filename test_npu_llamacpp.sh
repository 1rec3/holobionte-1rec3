#!/bin/bash
# Test NPU con llama.cpp (experimental)

echo "üß™ TEST NPU con llama.cpp"
echo ""

# Verifica si llama.cpp existe
if [ -d ~/holobionte/llama.cpp ]; then
    echo "‚úÖ llama.cpp encontrado"
    
    # Verifica build con XDNA
    if [ -f ~/holobionte/llama.cpp/build/bin/llama-cli ]; then
        echo "‚úÖ llama-cli encontrado"
        
        # Test NPU
        echo ""
        echo "üìä Intentando usar NPU..."
        export GGML_NPU=1
        
        # Necesitar√≠as modelo en GGUF format
        if [ -f ~/.ollama/models/blobs/sha256-* ]; then
            echo "‚ö†Ô∏è  Modelos Ollama en formato propietario"
            echo "   Necesitas convertir a GGUF para llama.cpp"
        fi
    else
        echo "‚ùå llama-cli no compilado"
    fi
else
    echo "‚ùå llama.cpp no instalado"
    echo "   Para NPU necesitas compilar con XRT support"
fi

echo ""
echo "üí° ESTADO NPU:"
echo "   - Hardware: ‚úÖ Disponible (XDNA 50 TOPS)"
echo "   - Driver: ‚ö†Ô∏è  B√°sico (sin optimizaci√≥n)"
echo "   - Software: ‚ùå No integrado en Ollama"
echo "   - Alternativa: llama.cpp + XRT (proyecto futuro)"
